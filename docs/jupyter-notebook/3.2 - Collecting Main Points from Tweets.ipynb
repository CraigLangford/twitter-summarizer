{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Collecting Main Points from Tweets\n",
    "\n",
    "From the previous section we built a comfortable understanding of collecting tweets from a specific hashtag. Utilizing tweepy and it's API any number of tweets could be collected. Nearly 50% of these tweets were original yielding a huge amount of different points and unique tweets that are relevant to the searched hashtag. In this section, these tweets will be filtered to and grouped to see if we can gather key points about certain events. Instead of selecting a specific tweet such as #Trump, let's use tweepy's API to find what's trending so this step is automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keys\n",
    "import tweepy\n",
    "from pprint import pprint\n",
    "consumer_key = keys.CONSUMER_KEY\n",
    "consumer_secret = keys.CONSUMER_SECRET\n",
    "access_token = keys.ACCESS_TOKEN\n",
    "access_token_secret = keys.ACCESS_TOKEN_SECRET\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From the tweepy documentation we can use api.trends_place which brings the trending tweets for a particular location. By passing 1 to this function it returns the trending tweets across the world.\n",
    "\n",
    ">**API.trends_place(id[, exclude])**\n",
    "\n",
    ">Returns the top 10 trending topics for a specific WOEID, if trending information is available for it.\n",
    "\n",
    ">The response is an array of “trend” objects that encode the name of the trending topic, the query parameter that can be used to search for the topic on Twitter Search, and the Twitter Search URL.\n",
    "\n",
    ">This information is cached for 5 minutes. Requesting more frequently than that will not return any more data, and will count against your rate limit usage.\n",
    "\n",
    ">Parameters:\t\n",
    ">id – The Yahoo! Where On Earth ID of the location to return trending information for. Global information is available by using 1 as the WOEID.\n",
    ">exclude – Setting this equal to hashtags will remove all hashtags from the trends list.\n",
    "\n",
    ">Return type: JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'as_of': '2017-04-06T07:47:44Z',\n",
      "  'created_at': '2017-04-06T07:47:12Z',\n",
      "  'locations': [...],\n",
      "  'trends': [...]}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(api.trends_place(1), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From the trending tweets we can see a dictionary is passed inside of a list object. Inside this dictionary various metadata about the request is included, as well as a key called 'trends' which contains a list of different trending tweets. #aprilfoolsdaycan be seen to have an enormous tweet volume at the moment (today is April 1st), however, many tweets are not in English. To try to narrow down the trending tweets to more relevant stories, let's see what's happening in Toronto! Looking up Toronto's WOEID shows that it is 4118. Let's pass that to the request instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'as_of': '2017-04-06T07:46:34Z',\n",
      "  'created_at': '2017-04-06T07:42:09Z',\n",
      "  'locations': [...],\n",
      "  'trends': [...]}]\n"
     ]
    }
   ],
   "source": [
    "toronto_trends = api.trends_place(4118)\n",
    "toronto_trends = toronto_trends[0]\n",
    "from pprint import pprint\n",
    "pprint(api.trends_place(1), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks like all the trending tweets are in English. Interestingly, these are not also specific to hashtags, therefore, Twitter may be able to find trending topics already for us which do not revolove around hashtags specifically. Let's see the properties of these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API returned 46 trending tweets when looking at Toronto.\n",
      "\n",
      "Sorted by volume, the top 10 are as follows:\n",
      "\n",
      "1 #TFCLive\n",
      "2 #15andFairness\n",
      "3 Sunshine List\n",
      "4 Game 5\n",
      "5 DeMar DeRozan\n",
      "6 Pacers\n",
      "7 Toronto FC\n",
      "8 #JUNOS\n",
      "9 #IllNeverBeTooOldFor\n",
      "10 #BlueJaysMTL\n"
     ]
    }
   ],
   "source": [
    "print(\"The API returned {} trending tweets when looking at Toronto.\\n\".format(len(toronto_trends['trends'])))\n",
    "sorted_trends = sorted(toronto_trends['trends'], key=lambda x: x['tweet_volume'] if x['tweet_volume'] else 0)\n",
    "print(\"Sorted by volume, the top 10 are as follows:\\n\")\n",
    "for i in range(10):\n",
    "    print(i+1, sorted_trends[i]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As seen from the tweets, many relevant tweets specific to Toronto can be seen. This includes the Toronto football club (TFC), the minimum wage (15 and fairness) as well as the Sunshine List (annual report on any Canadian public worker earning $100,000+). Let's grab the top tweets on TFCLive, dump them in a pickle and create a word cloud following section 3.1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "full_tfc_statuses = [status for status in tweepy.Cursor(api.search, '#TFCLive', tweet_mode='extended', languages=[\"en\"]).items(1000)]\n",
    "pickle.dump(full_tfc_statuses, open('pickle_dumps/full_tfc_statuses.p', 'wb'))\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pickle, re\n",
    "\n",
    "# Get all tweets\n",
    "full_tfc_statuses = pickle.load(open('pickle_dumps/full_tfc_statuses.p', 'rb'))\n",
    "# Convert retweets to original tweet\n",
    "original_full_tweets = [t.retweeted_status  if 'retweeted_status' in dir(t) else t\n",
    "                           for t in full_tfc_statuses]\n",
    "pickle.dump(original_full_tweets, open('pickle_dumps/original_full_tfc_tweets.p', 'wb'))\n",
    "\n",
    "# Clean tweets\n",
    "used_tweets = set()\n",
    "full_tweet_text = []\n",
    "for tweet in original_full_tweets:\n",
    "    if tweet.id not in used_tweets:\n",
    "        used_tweets.add(tweet.id)\n",
    "        # Obtain text and remove urls\n",
    "        cleaned_tweet = re.sub(\"http\\S+\", \"\", tweet.full_text)\n",
    "        full_tweet_text.append(cleaned_tweet)\n",
    "        \n",
    "cleaner_text_dump = ' '.join(full_tweet_text)\n",
    "pickle.dump(cleaner_text_dump, open('pickle_dumps/tfc_tweets_text_dump.p', 'wb'))\n",
    "\n",
    "# We'll make the words into a sillhoute of a soccer player\n",
    "# Found at https://www.pinterest.com/pin/469992911093328088/\n",
    "football_mask = np.array(Image.open(\"images/football_mask.png\"))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('TFCLive')\n",
    "stopwords.add('amp')\n",
    "stopwords.add('rt')\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=football_mask,\n",
    "               stopwords=stopwords)\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(cleaner_text_dump)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"images/football_word_cloud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The resulting wordcloud from repeating these results are as below.\n",
    "<img src=\"images/football_word_cloud.png\" alt=\"TFC Wordcloud\" width=400>\n",
    "TORvSKC can be seen to be the top trending word (likely a hashtag), so let's filter tweets containing this text to filter down our tweets to relevant information. We'll first find TORvSKC via the natural language processing tool kit (NLTK) to find the most common words numerically so that this step can be automated in the future. Let's install NLTK.\n",
    "```\n",
    "$ pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the text dump\n",
    "tfc_tweets_text_dump = pickle.load(open('pickle_dumps/tfc_tweets_text_dump.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
